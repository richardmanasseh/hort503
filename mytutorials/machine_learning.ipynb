{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Programming: Machine Learning\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Machine Learning? \n",
    "   # Humans can learn from past experiences\n",
    "   # Can computers learn from past experience? Yes,in this case the past experience is called (previous) data.\n",
    "   # Machine Learning is teaching computers to learn to perform tasks from past experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Machine learning is a method of data analysis that automates analytical model building. \n",
    "It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\n",
    "the machine is \"learning\" from data, i.e. updating parameters/beliefs/etc based on data.\n",
    "\n",
    "In order to make matters simpler, you feed in this data to a computer, and it does the donkey work for you:\n",
    "sorting through the numbers, clustering similar ones(unsupervised learning) , \n",
    "identifying those which don't make sense(outlier detection), identifying existing properties which tie the numbers together in a peculiar way(curve fitting), \n",
    "discovering repetitive patterns in different groups of data(training and classification), etc.\n",
    "Hence, this branch is hence termed computer learning , oops!, machine learning of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Example 1: Price of a house\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Let's suppose we're studying the housing market\n",
    "    # Our task is to predict the size of a house based on it's size\n",
    "    # We've a small house valued at $70,000 and a big house valued at $160,000\n",
    "    # We'd like to predict the price of a medium house, given these data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach:\n",
    "    # Put the data in a grid (<=> scatter plot): size (x) vs price (y)\n",
    "    # To make the best guess of the price of the house, we can first make a line through the points\n",
    "    # Our best guess would then be the point on the line corresponding to the medium house\n",
    "    # ...this is called linear regression\n",
    "    # But how do we find this line? Get the computer to draw this line, then evaluate how bad the line is\n",
    "    # To see how bad the line is, we calculate the Error\n",
    "    # To calculate the Error, we look at the length of distances of the points from the line and add them\n",
    "    # We again move re-orient this line to see if we can reduce this Error. We get the distances and add them. \n",
    "    # This may increase the direction; if it does, is not a good direction to go, so we move the line in the opposite diretion\n",
    "    # If this reduces the Error, we can repeat the adjustment, until we find the best fitting line: a minimum value for Error. This is called gradient descent ? \n",
    "    # What is gradient descent ? It is an optimization algorithm to find the minimum of a function.\n",
    "    # In real life, we dont want to deal with negative distances, so we compute Error as the sum of squares of the distances, a procedure called least squares\n",
    "    # We could also draw a circle, parabola, or higher-degree curve, etc to the data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Detecting Spam Emails with Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we are going to build an email spam detection classifier..something that will tell us if an email is spam or not\n",
    "# Our previous data is 100 emails we have looked at already, out of which we have flagged 25 as spam and 75 as non-spam\n",
    "# Now we try to think of features of spam email may be likely to display. Let's say spam email is likely to contain the word \"cheap\",\n",
    "    # We then analyse this claim: we look for the word \"cheap\" in our 100 emails. \n",
    "    # Suppose we find 20 of the spam ones and 5 out of the spam ones contain the word cheap\n",
    "    # Focusing on the emails that contain the word \"cheap\", we can then ask, if an email contains the word \"cheap\", what is the probabilty of it being spam?\n",
    "        # which is 20/25*100 = 80%\n",
    "        # so we are going to associate this probability with the word \"cheap\" and use it to flag future emails\n",
    "        # so now when future emails come, we can also look at other features e.g. having spelling mistakes, missing titles and find their associated probabilities \n",
    "        # we can compine these features to guess whethere they'are spam or not. This algorithm is called the Naive Bayes Algorithm\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Example 3: Recommedning Apps with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've gathered data of 6 people: showing their age, gender and app downloaded\n",
    "# data = {\"age\": 15, 25, 32, 40, 12, 14,\n",
    "      # \"gender\" : \"F, F, M, F, M, M\",\n",
    "       # \"app\" : \"pg\", \"watsapp\", \"fb\", \"watsapp\", \"pg\", \"pg\"}\n",
    "\n",
    "# We want to know which is a better/the most decisive predictor of choice of app users will download: age or gender?\n",
    "\n",
    "# Splitting by gender: Females downloaded pg and watsapp, males downloaded pg and fb, so not much clear-cut split by gender\n",
    "# Splitting by age: everybody under 20 downloaded pg, everybody above 20 didn't\n",
    "# So the feature that best splits the data is age\n",
    "\n",
    "# So were going to start by considering age:\n",
    "    # if <20, recommend pg\n",
    "    # if >20, we then consider the gender\n",
    "    # if female, we recommend \"watsapp\"; if male we recommend \"fb\"\n",
    "    \n",
    "# We're going to find algorithm for the best tree for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Example 4: Acceptance at a University: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose that students are admitted to a university based on two pieces of information: gre scores and grade from high school\n",
    "    # Student 1: gre score 9/10, grade 8/10 --> accepted\n",
    "    # Student 2: gre score 3/10, grade 4/10 --> rejected\n",
    "    # Student 3: gre score 7/10, grade 6/10 --> ???\n",
    "\n",
    "# To decide if student 3 should be accepted, we can first put this data in a grid of gre score vs grade for all students that have been accepted and rejected\n",
    "# so we can try to find a trend in the data\n",
    "    # Case I: data cloudy with no apparent trend, yet a line could be fitted that places one category below and the other above it (with a few exceptions on either side)\n",
    "    # This line places student 3 in the category of students that get accepted, so we predict this student gets accepted\n",
    "    # This method is called losistic regression\n",
    "# How do we find this line that best cuts the data in two?\n",
    "    # Again the computer can start by drawing a random line and measure how bad the line is by counting the mis-classified points as Error\n",
    "    # Like in linear regression, we'd then move the line in either gradient (+-) to try bring the mis-classification to a minimum using gradient descent\n",
    "    # In reality, since we use calculus for our gradient descent method, it turns out the number of errors is not what we need to minimize..\n",
    "    # Rather, something that captures the number of errors, called the log-loss function\n",
    "    # THe idea of the log-loss function is that it assigns a large value (penalty) to the mis-classified points (bacause the add a lot to the error function, Error) and a small value (penalty) to the classified points\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes, a circle or (an intersection of) two lines, rather than just one line, are needed to accurately delineate the points denoting acceptance\n",
    "# How do we find these two lines? Again, we can do it using gradient descent to minimize a similar log-loss function as before. This returns \n",
    "# The region of acceptance is now constructed as an intersection of two lines.\n",
    "# The two lines..a neural network..divide the data points into four regions\n",
    "# So the decision to accept would be derived from three questions:\n",
    "    # 1 Is the point over the blue line -->y/n\n",
    "    # 2 Is the point over the yelow line -->y/n\n",
    "    # 3 Are the answere to 1 and 2 both yes? -->y/n\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5: Support Vector Machine with linear optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's consider the art of splitiing data into two\n",
    "# In most cases, there will be many possible lines which can split the data into two. which would be the best?\n",
    "# Possibly one nicely spaced from most of the points on either side\n",
    "# So we're noing to use, not gradient descent, but linear optimization to find the line that maximizes the distance from the boundary points\n",
    "# This method is called a Support Vector Machine \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When a line is not enough: the Kernel trick: planes for curves and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(0,3), (1,2), (2,1), (3,0)\n",
    "# A line wont split the data since they're already on the line, and the two red are in the middle\n",
    "# We could use a curve to split them\n",
    "# Another one is to actually think outside the x-y plane, and think of the points as lying in a 3D space\n",
    "# by placing (0,3) and (3,0) exactly on the y and x-axis, respectively, we can he lift (1,2) and (2,1) along the z-axis.\n",
    "# then we'd be able to separate them with a plane\n",
    "# \n",
    "# Use of a curve or  a plane are actually the same things. We call This method the Kernel Trick, and is well used in Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the curve trick:\n",
    "# What we need is a way to separate the green points from the red points.\n",
    "# To do so, we need an equation on variables x and why that gives us large values for the middle(green) points and low values for the points at the end (or vice versa)\n",
    "# But which equastion? X**2? xy? x+y?\n",
    "# Apply each equation on the points?\n",
    "    # x+y gives a sum of 3 for all the values => doesn't separate them\n",
    "    # \n",
    "    # x**2 gives 0, 1, 4, 9 => different values for every point, but doesnt really tell them apart\n",
    "    # xy gives 0, 2, 2, 0 => tells them apart\n",
    "        # for the red points, xy = 0\n",
    "        # for the green points, xy = 2\n",
    "        # what is in between 0 and 2? A one (1). So the line xy = 1 will separate the two sets of points\n",
    "        # xy = 1 is also the same as y = 1/x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Considering (0,3), (1,2), (2,1), (3,0) in 3D space: as (x, y, xy), with xy as height (z-axis)\n",
    "# (0,3) => (0, 3, 0)\n",
    "# (1,2) => (1, 2, 2)\n",
    "# (2,1) => (2, 1, 2)\n",
    "# (3,0) => (3, 0, 0)\n",
    "\n",
    "# (1, 2, 2) and (2, 1, 2) with fall in the same x-y plane\n",
    "# (0, 3, 0) and (3, 0, 0) will also fall in theiw own x-y plane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Say we want to establish a chain of pizza parlors \n",
    "# We want to put three of them in the city\n",
    "# We make a study and realize that people who love pizzas most live in three distinct locations of the city\n",
    "# Where are the optimal places to put our pizza parlors?\n",
    "# It seeems that the houses surveyed seem nicely split into three groups, so it would make sense to put one parlor in each location, but a computer doesnt know how to do this,\n",
    "# and cannot just eyeball the groups (eyeball as verb = look at somebody/something in a way that is very direct)\n",
    "# so we need an algorithm\n",
    "# We start by putting three random locations for the pizza parlors\n",
    "# What should make sense is that each house should go to the pizza palor closest to it\n",
    "# In that case, we'd try to move the parlors to the center of the houses they will serve\n",
    "# In doing so, we'd however, have moved the pizza palors away from some of the houses it was mean't to serve, and in turn, bring some others closer\n",
    "# So we have to re-cluster (re-associate) the houses and then move the parlors to new central locations\n",
    "# We can iterate this process until we reach the best possible clustering and positions for the pizza parlors\n",
    "# THis algorithm is called K-Means clustering\n",
    "# K-Means clustering is useful when we have an idea of how many clusters we want to end up with\n",
    "# What if we don't know how many clusters we want to end up with?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierachical Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It would still make sense that if two houses are close, they should be served by the same pizza parlor\n",
    "# So going by this philosophy, we can try to group the houses\n",
    "# We can start with two houses that are closest to each other and group them\n",
    "# We then join the next two closest, and also join them\n",
    "# The next two closest houses may be one ungrouped joined to another already in a group, or both already in two different groups, so we regroup\n",
    "# We'd repeat the process until the next two houses are too from each other so we stop there.\n",
    "# What we've done is picked a distance to begin with, and then said, whenever the two cloeset houses aare these distance apart, then we stop the algorithm.\n",
    "# THat will control how far we want the clusters to be apart\n",
    "# THis algorithm is called K-Means clustering\n",
    "# Hierachical Clustering is useful when we may not know the number of clusters and we have no idea how far we want them to be \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
