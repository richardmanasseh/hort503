{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8, Part 1: Pandas\n",
    "\n",
    "This notebook is based on the official `pandas` [documentation](https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html).  Unless otherwise credited, quoted text comes from this document.\n",
    "\n",
    "`Pandas` is a Python package that we will use to manage labeld data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "*Where does this tool fit in my life?*\n",
    "\n",
    "> For data scientists, working with data is typically divided into multiple stages: munging and cleaning data, analyzing / modeling it, then organizing the results of the analysis into a form suitable for plotting or tabular display. pandas is the ideal tool for all of these tasks.\n",
    "> munging: the process of changing data into another format (= arrangement) so that it can be used or processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Why use this package?*\n",
    "\n",
    "> pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures\n",
    "\n",
    "Pandas provides two new objects in which to store and manipulate data.\n",
    "\n",
    "Dimensions | Name |\tDescription\n",
    "-----------|-------|--------------------------------------\n",
    "1 |\tSeries | \t1D labeled homogeneously-typed array\n",
    "2 |\tDataFrame | \tGeneral 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column\n",
    "\n",
    "Typical usage of this package involves the creation of one or more `DataFrame` objects, and manipulating them with the provided methods.\n",
    "\n",
    "\n",
    "Technically, Pandas Series is a one-dimensional labeled array capable of holding any data type. In layman terms, Pandas Series is nothing but a column in an excel sheet. Columns with Name, Age and Designation representing a Series. So, in terms of Pandas DataStructure, A Series represents a single column in memory, which is either independent or belongs to a Pandas DataFrame. A Series can have its own independent existence without being part of a DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Cheat Sheet\n",
    "\n",
    "*\"[`Package Name`] cheat sheet\"* for popular packages can often be a worthwhile internet query.\n",
    "\n",
    "In the case of pandas, there is an official cheet sheat you can find [here](http://pandas.pydata.org/Pandas_Cheat_Sheet.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10* Minutes to Pandas\n",
    "\n",
    "The official [10 Minutes to Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html#min) documentation is an excellent resource, and provides the titular guide which we will drawn from and expanded upon for this tutorial.\n",
    "\n",
    "_Note: You can access the pandas documentation from within a jupyter lab session by selecting 'help' and then selecting 'pandas' from the dropdown menu._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Getting Started\n",
    "\n",
    "First, we need to import the pandas library (and Numpy library too).  All packages are imported at the top of the notebook. Execute the code in the following cell to get started with this notebook (type Ctrl+Enter in the cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy and pandas usage are often intertwined. \n",
    "# These abbreviations (=nicknames) are ubiquitiously used.\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above imports both numpy and pandas as variables named `np` and `pd`. We can use these variables to access the functionality of both libraries respectively.  The above is what we will use for the rest of this class.\n",
    "\n",
    "You may be wondering why we didn't import pandas like this:  \n",
    "```python\n",
    "import pandas\n",
    "```\n",
    "We could, but the first is far more commonly seen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.1: Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Object Creation\n",
    "\n",
    "Here we will learn to create and use the two data structures provided by Pandas: **Series** and **DataFrames**.  For additional help, see the [Data Structure Intro section](https://pandas.pydata.org/pandas-docs/stable/getting_started/dsintro.html#dsintro).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The `pd.Series` Data Object\n",
    "\n",
    "In Pandas, a Series is a one dimensional array, where all the entries are of the same data type (i.e. integer, string, boolean, etc.) or **NaN**.  NaN means \"not a number\" but serves as a marker when a value is missing.  A Pandas Series can be created out of a Python list or NumPy array. Before using a series take care to consider the correct data type.\n",
    "\n",
    "The following cell creates an new series containing an array of 5 numbers and one missing value by simply passing a list of 6 elements to the `pd.Series` function. Execute the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    3.0\n",
       "2    5.0\n",
       "3    NaN\n",
       "4    6.0\n",
       "5    8.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the series object.\n",
    "\n",
    "my_series = pd.Series([1, 3, 5, np.nan, 6, 8])\n",
    "my_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the next cell to see what the new series object named `my_series` contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    3.0\n",
       "2    5.0\n",
       "3    NaN\n",
       "4    6.0\n",
       "5    8.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the object in the notebook.\n",
    "# This (mostly) calls the __repr__() function of a given object.\n",
    "my_series # also generates by default row index numbers which is a sequence of incremental numbers starting from ‘0’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2a Create a `pd.Series` object\n",
    "Use the practice notebook to create a series of your own design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The `pd.DataFrame` Data Object\n",
    "\n",
    "In Pandas, a DataFrame is a two-dimensional array.  Just like Numpy, it has axes labeled \"rows\" and \"cols\".  However, unlike Numpy, it allows for different data types in each column!  DataFrames also provide greater flexibilty for indexing and slicing.\n",
    "\n",
    "#### 2.2.1 Create a DataFrame with a dictionary\n",
    "\n",
    "A dataframe is created by passing a Python dictionary (i.e. **dict**) to the `pd.DataFrame` function.  Remember dictionaries have key/value pairs. The dictionary is interpreted in the following way:\n",
    "\n",
    "- The keys in the dictionary argument becomes the column names\n",
    "- The value in the dictionary argument contains the column values\n",
    "- Each \"column\" must have the same number of values.\n",
    "\n",
    "The following cell contains code that creates a new DataFrame object named `df_1`.  This dataframe consists of two columns, one named \"alpha\" and the other \"beta\".  Each column has 5 elements. Notice that the columns contain different data types.  Execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha beta\n",
       "0      0    a\n",
       "1      1    b\n",
       "2      2    c\n",
       "3      3    d\n",
       "4      4    e"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe with a dictionary.\n",
    "df_1 = pd.DataFrame(\n",
    "    {'alpha': [0, 1, 2, 3, 4],\n",
    "     'beta': ['a', 'b', 'c', 'd', 'e']}) \n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the next cell to see what the new data frame object named `df_1` contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha beta\n",
       "0      0    a\n",
       "1      1    b\n",
       "2      2    c\n",
       "3      3    d\n",
       "4      4    e"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the object in the notebook.\n",
    "# This time we get a html output, as the notebook shell\n",
    "# sees a _repr_html_() function to call.\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that a DataFrame can be visualized as a data table or Excel spreadsheet. Also, an index, in the left-most column, has been automatically generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Create a DataFrame with a Numpy array\n",
    "Alternatively, we can create a DataFrame by passing in a Numpy matrix.  Remember that this is only possible if the data is all of the same type as Numpy arrays can only store one data type. The following code creates a Dataframe of 6 rows and 4 columns and fills it with random numbers generated using the Numpy `np.random.randn` function.  Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(np.random.randn(6, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our new data frame by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.188021</td>\n",
       "      <td>0.358576</td>\n",
       "      <td>0.199851</td>\n",
       "      <td>-0.755414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.434187</td>\n",
       "      <td>2.011562</td>\n",
       "      <td>0.916710</td>\n",
       "      <td>0.211892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.188432</td>\n",
       "      <td>0.243916</td>\n",
       "      <td>0.103573</td>\n",
       "      <td>1.694751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.867412</td>\n",
       "      <td>0.429264</td>\n",
       "      <td>-0.560389</td>\n",
       "      <td>1.705810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.605467</td>\n",
       "      <td>0.820141</td>\n",
       "      <td>-0.339101</td>\n",
       "      <td>-2.009380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.233016</td>\n",
       "      <td>-0.789233</td>\n",
       "      <td>1.813728</td>\n",
       "      <td>0.913068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0  1.188021  0.358576  0.199851 -0.755414\n",
       "1  0.434187  2.011562  0.916710  0.211892\n",
       "2 -1.188432  0.243916  0.103573  1.694751\n",
       "3  0.867412  0.429264 -0.560389  1.705810\n",
       "4 -1.605467  0.820141 -0.339101 -2.009380\n",
       "5 -0.233016 -0.789233  1.813728  0.913068"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that because we did not provide a dictionary, we do not have header names for each column. Rather a numeric index was automatically generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2b: Create a pd.DataFrame object from a Python dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Create a DataFrame with row and column labels\n",
    "\n",
    "Rather than have a numeric index for each row of the DataFrame, we may want to give each row a human-readable label. This will help make the data more accessible (as will be seen later).  \n",
    "\n",
    "To demonstrate, lets reuse the 6x4 datafame we created in section 2.2.2.  First, we need to provide our index names.  Suppose we want our row names to be dates, perhaps corresponding to when the row data was collected. The code below uses the `pd.date_range` function to do create a list of dates. If this were real data we would most likely include the dates collected as a column in the table, but for the purposes of demonstration we'll generate a series of regularly spaces dates over a 6 day period: one for each of the 6 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04',\n",
       "               '2013-01-05', '2013-01-06'],\n",
       "              dtype='datetime64[ns]', freq='D')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = pd.date_range('20130101', periods=6)\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also suppose that we have 4 columns and we want them named 'A', 'B', 'C', and 'D'. \n",
    "\n",
    "We can re-create the DataFrame from section 2.2.2 with these names for the columns and the dates as row indexes by using the `index` and `columns` arguments when creating the DataFram as shown in the following cell.  Execute the cell to see the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>0.785379</td>\n",
       "      <td>0.923016</td>\n",
       "      <td>-1.240253</td>\n",
       "      <td>-1.533414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>0.262330</td>\n",
       "      <td>0.851739</td>\n",
       "      <td>-0.706875</td>\n",
       "      <td>0.146317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>-2.544173</td>\n",
       "      <td>-0.459463</td>\n",
       "      <td>0.694887</td>\n",
       "      <td>-0.431645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>3.038082</td>\n",
       "      <td>1.057172</td>\n",
       "      <td>0.156867</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-05</th>\n",
       "      <td>-0.488879</td>\n",
       "      <td>-0.072836</td>\n",
       "      <td>-0.699568</td>\n",
       "      <td>1.664431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-06</th>\n",
       "      <td>0.374017</td>\n",
       "      <td>0.628792</td>\n",
       "      <td>0.054729</td>\n",
       "      <td>0.746433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   A         B         C         D\n",
       "2013-01-01  0.785379  0.923016 -1.240253 -1.533414\n",
       "2013-01-02  0.262330  0.851739 -0.706875  0.146317\n",
       "2013-01-03 -2.544173 -0.459463  0.694887 -0.431645\n",
       "2013-01-04  3.038082  1.057172  0.156867  0.476676\n",
       "2013-01-05 -0.488879 -0.072836 -0.699568  1.664431\n",
       "2013-01-06  0.374017  0.628792  0.054729  0.746433"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2c: Supply indexes and columns when creating a pd.DataFrame object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Loading Data\n",
    "\n",
    "Creating a DataFrame manually, as in the previous examples, would be tedious for large datasets.  Fortunately, Pandas provides a variety of functions for importing data from files.  This saves you from writing your own Python code to open a file, read in the contents and format it for inclusion into a DataFrame. For this tutorial we will look at two of these import functions:  `pd.read_csv` and `pd.read_excel()`.\n",
    "\n",
    "For this portion of the tutorial, we will use a rather [famous set of data concerning iris flowers](https://en.wikipedia.org/wiki/Iris_flower_data_set).  You can find a copy of the file [by clicking here]( https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/d546eaee765268bf2f487608c537c05e22e4b221/iris.csv).\n",
    "\n",
    "![Iris flower](https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/193px-Iris_versicolor_3.jpg)\n",
    "\n",
    "To use this file, create a directory named `data` in the same directory as this notebook and save the file in that directory with the name `iris.csv`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3a: Download the iris.csv file.\n",
    "Download the iris data and take a moment to learn about this data from the Wikipedia page cited above\n",
    "# Compares morphologic variation of Iris flowers of three related species \n",
    "# The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). \n",
    "# Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. \n",
    "# Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Importing Using `pd.read_csv`\n",
    "\n",
    "The `pd.read_csv` function can be used to read in files that are in comma-separated format or tab delimited format: two very common data formats.  This function is very flexible and has a large number of arguments. See the [`pd.read_csv` online documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) for a thorough listing of arguments. \n",
    "\n",
    "The iris data is in CSV format and is compatible with the default arguments for the `pd.read_csv` function, therefore we can easily import this data by providing the path to the file as the only argument to the function. This will automatically create a DataFrame for us! Execute the following line to import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv('data/iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3b: Import the iris.csv file\n",
    "In your own practice notebook, import the iris dataset.\n",
    "\n",
    "#### Task 3c: Import a tab delimited file\n",
    "Take a look at the `pd.read_csv` online documentation. Describe how you would import a tab-delimited file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_csv in module pandas.io.parsers:\n",
      "\n",
      "read_csv(filepath_or_buffer, sep=',', delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression='infer', thousands=None, decimal=b'.', lineterminator=None, quotechar='\"', quoting=0, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=None, error_bad_lines=True, warn_bad_lines=True, skipfooter=0, doublequote=True, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None)\n",
      "    Read CSV (comma-separated) file into DataFrame\n",
      "    \n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "    \n",
      "    Additional help can be found in the `online docs for IO Tools\n",
      "    <http://pandas.pydata.org/pandas-docs/stable/io.html>`_.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, pathlib.Path, py._path.local.LocalPath or any \\\n",
      "    object with a read() method (such as a file handle or StringIO)\n",
      "        The string could be a URL. Valid URL schemes include http, ftp, s3, and\n",
      "        file. For file URLs, a host is expected. For instance, a local file could\n",
      "        be file://localhost/path/to/table.csv\n",
      "    sep : str, default ','\n",
      "        Delimiter to use. If sep is None, the C engine cannot automatically detect\n",
      "        the separator, but the Python parsing engine can, meaning the latter will\n",
      "        be used and automatically detect the separator by Python's builtin sniffer\n",
      "        tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n",
      "        different from ``'\\s+'`` will be interpreted as regular expressions and\n",
      "        will also force the use of the Python parsing engine. Note that regex\n",
      "        delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``\n",
      "    delimiter : str, default ``None``\n",
      "        Alternative argument name for sep.\n",
      "    delim_whitespace : boolean, default False\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n",
      "        used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "        is set to True, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "    \n",
      "        .. versionadded:: 0.18.1 support for the Python parser.\n",
      "    \n",
      "    header : int or list of ints, default 'infer'\n",
      "        Row number(s) to use as the column names, and the start of the\n",
      "        data.  Default behavior is to infer the column names: if no names\n",
      "        are passed the behavior is identical to ``header=0`` and column\n",
      "        names are inferred from the first line of the file, if column\n",
      "        names are passed explicitly then the behavior is identical to\n",
      "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "        replace existing names. The header can be a list of integers that\n",
      "        specify row locations for a multi-index on the columns\n",
      "        e.g. [0,1,3]. Intervening rows that are not specified will be\n",
      "        skipped (e.g. 2 in this example is skipped). Note that this\n",
      "        parameter ignores commented lines and empty lines if\n",
      "        ``skip_blank_lines=True``, so header=0 denotes the first line of\n",
      "        data rather than the first line of the file.\n",
      "    names : array-like, default None\n",
      "        List of column names to use. If file contains no header row, then you\n",
      "        should explicitly pass header=None. Duplicates in this list will cause\n",
      "        a ``UserWarning`` to be issued.\n",
      "    index_col : int or sequence or False, default None\n",
      "        Column to use as the row labels of the DataFrame. If a sequence is given, a\n",
      "        MultiIndex is used. If you have a malformed file with delimiters at the end\n",
      "        of each line, you might consider index_col=False to force pandas to _not_\n",
      "        use the first column as the index (row names)\n",
      "    usecols : list-like or callable, default None\n",
      "        Return a subset of the columns. If list-like, all elements must either\n",
      "        be positional (i.e. integer indices into the document columns) or strings\n",
      "        that correspond to column names provided either by the user in `names` or\n",
      "        inferred from the document header row(s). For example, a valid list-like\n",
      "        `usecols` parameter would be [0, 1, 2] or ['foo', 'bar', 'baz']. Element\n",
      "        order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "        To instantiate a DataFrame from ``data`` with element order preserved use\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n",
      "        in ``['foo', 'bar']`` order or\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "        for ``['bar', 'foo']`` order.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the column\n",
      "        names, returning names where the callable function evaluates to True. An\n",
      "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "        parsing time and lower memory usage.\n",
      "    squeeze : boolean, default False\n",
      "        If the parsed data only contains one column then return a Series\n",
      "    prefix : str, default None\n",
      "        Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...\n",
      "    mangle_dupe_cols : boolean, default True\n",
      "        Duplicate columns will be specified as 'X', 'X.1', ...'X.N', rather than\n",
      "        'X'...'X'. Passing in False will cause data to be overwritten if there\n",
      "        are duplicate names in the columns.\n",
      "    dtype : Type name or dict of column -> type, default None\n",
      "        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32}\n",
      "        Use `str` or `object` together with suitable `na_values` settings\n",
      "        to preserve and not interpret dtype.\n",
      "        If converters are specified, they will be applied INSTEAD\n",
      "        of dtype conversion.\n",
      "    engine : {'c', 'python'}, optional\n",
      "        Parser engine to use. The C engine is faster while the python engine is\n",
      "        currently more feature-complete.\n",
      "    converters : dict, default None\n",
      "        Dict of functions for converting values in certain columns. Keys can either\n",
      "        be integers or column labels\n",
      "    true_values : list, default None\n",
      "        Values to consider as True\n",
      "    false_values : list, default None\n",
      "        Values to consider as False\n",
      "    skipinitialspace : boolean, default False\n",
      "        Skip spaces after delimiter.\n",
      "    skiprows : list-like or integer or callable, default None\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
      "        at the start of the file.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the row\n",
      "        indices, returning True if the row should be skipped and False otherwise.\n",
      "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with engine='c')\n",
      "    nrows : int, default None\n",
      "        Number of rows of file to read. Useful for reading pieces of large files\n",
      "    na_values : scalar, str, list-like, or dict, default None\n",
      "        Additional strings to recognize as NA/NaN. If dict passed, specific\n",
      "        per-column NA values.  By default the following values are interpreted as\n",
      "        NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
      "        '1.#IND', '1.#QNAN', 'N/A', 'NA', 'NULL', 'NaN', 'n/a', 'nan',\n",
      "        'null'.\n",
      "    keep_default_na : bool, default True\n",
      "        Whether or not to include the default NaN values when parsing the data.\n",
      "        Depending on whether `na_values` is passed in, the behavior is as follows:\n",
      "    \n",
      "        * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n",
      "          is appended to the default NaN values used for parsing.\n",
      "        * If `keep_default_na` is True, and `na_values` are not specified, only\n",
      "          the default NaN values are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are specified, only\n",
      "          the NaN values specified `na_values` are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are not specified, no\n",
      "          strings will be parsed as NaN.\n",
      "    \n",
      "        Note that if `na_filter` is passed in as False, the `keep_default_na` and\n",
      "        `na_values` parameters will be ignored.\n",
      "    na_filter : boolean, default True\n",
      "        Detect missing value markers (empty strings and the value of na_values). In\n",
      "        data without any NAs, passing na_filter=False can improve the performance\n",
      "        of reading a large file\n",
      "    verbose : boolean, default False\n",
      "        Indicate number of NA values placed in non-numeric columns\n",
      "    skip_blank_lines : boolean, default True\n",
      "        If True, skip over blank lines rather than interpreting as NaN values\n",
      "    parse_dates : boolean or list of ints or names or list of lists or dict, default False\n",
      "    \n",
      "        * boolean. If True -> try parsing the index.\n",
      "        * list of ints or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
      "          each as a separate date column.\n",
      "        * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
      "          a single date column.\n",
      "        * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call result\n",
      "          'foo'\n",
      "    \n",
      "        If a column or index contains an unparseable date, the entire column or\n",
      "        index will be returned unaltered as an object data type. For non-standard\n",
      "        datetime parsing, use ``pd.to_datetime`` after ``pd.read_csv``\n",
      "    \n",
      "        Note: A fast-path exists for iso8601-formatted dates.\n",
      "    infer_datetime_format : boolean, default False\n",
      "        If True and `parse_dates` is enabled, pandas will attempt to infer the\n",
      "        format of the datetime strings in the columns, and if it can be inferred,\n",
      "        switch to a faster method of parsing them. In some cases this can increase\n",
      "        the parsing speed by 5-10x.\n",
      "    keep_date_col : boolean, default False\n",
      "        If True and `parse_dates` specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : function, default None\n",
      "        Function to use for converting a sequence of string columns to an array of\n",
      "        datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "        conversion. Pandas will try to call `date_parser` in three different ways,\n",
      "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n",
      "        string values from the columns defined by `parse_dates` into a single array\n",
      "        and pass that; and 3) call `date_parser` once for each row using one or\n",
      "        more strings (corresponding to the columns defined by `parse_dates`) as\n",
      "        arguments.\n",
      "    dayfirst : boolean, default False\n",
      "        DD/MM format dates, international and European format\n",
      "    iterator : boolean, default False\n",
      "        Return TextFileReader object for iteration or getting chunks with\n",
      "        ``get_chunk()``.\n",
      "    chunksize : int, default None\n",
      "        Return TextFileReader object for iteration.\n",
      "        See the `IO Tools docs\n",
      "        <http://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "        for more information on ``iterator`` and ``chunksize``.\n",
      "    compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer' and\n",
      "        `filepath_or_buffer` is path-like, then detect compression from the\n",
      "        following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n",
      "        decompression). If using 'zip', the ZIP file must contain only one data\n",
      "        file to be read in. Set to None for no decompression.\n",
      "    \n",
      "        .. versionadded:: 0.18.1 support for 'zip' and 'xz' compression.\n",
      "    \n",
      "    thousands : str, default None\n",
      "        Thousands separator\n",
      "    decimal : str, default '.'\n",
      "        Character to recognize as decimal point (e.g. use ',' for European data).\n",
      "    float_precision : string, default None\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are `None` for the ordinary converter,\n",
      "        `high` for the high-precision converter, and `round_trip` for the\n",
      "        round-trip converter.\n",
      "    lineterminator : str (length 1), default None\n",
      "        Character to break file into lines. Only valid with C parser.\n",
      "    quotechar : str (length 1), optional\n",
      "        The character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the delimiter and it will be ignored.\n",
      "    quoting : int or csv.QUOTE_* instance, default 0\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
      "        QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
      "    doublequote : boolean, default ``True``\n",
      "       When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n",
      "       whether or not to interpret two consecutive quotechar elements INSIDE a\n",
      "       field as a single ``quotechar`` element.\n",
      "    escapechar : str (length 1), default None\n",
      "        One-character string used to escape delimiter when quoting is QUOTE_NONE.\n",
      "    comment : str, default None\n",
      "        Indicates remainder of line should not be parsed. If found at the beginning\n",
      "        of a line, the line will be ignored altogether. This parameter must be a\n",
      "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter `header` but not by\n",
      "        `skiprows`. For example, if ``comment='#'``, parsing\n",
      "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n",
      "        treated as the header.\n",
      "    encoding : str, default None\n",
      "        Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_\n",
      "    dialect : str or csv.Dialect instance, default None\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
      "        `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
      "        override values, a ParserWarning will be issued. See csv.Dialect\n",
      "        documentation for more details.\n",
      "    tupleize_cols : boolean, default False\n",
      "        .. deprecated:: 0.21.0\n",
      "           This argument will be removed and will always convert to MultiIndex\n",
      "    \n",
      "        Leave a list of tuples on columns as is (default is to convert to\n",
      "        a MultiIndex on the columns)\n",
      "    error_bad_lines : boolean, default True\n",
      "        Lines with too many fields (e.g. a csv line with too many commas) will by\n",
      "        default cause an exception to be raised, and no DataFrame will be returned.\n",
      "        If False, then these \"bad lines\" will dropped from the DataFrame that is\n",
      "        returned.\n",
      "    warn_bad_lines : boolean, default True\n",
      "        If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n",
      "        \"bad line\" will be output.\n",
      "    low_memory : boolean, default True\n",
      "        Internally process the file in chunks, resulting in lower memory use\n",
      "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "        types either set False, or specify the type with the `dtype` parameter.\n",
      "        Note that the entire file is read into a single DataFrame regardless,\n",
      "        use the `chunksize` or `iterator` parameter to return the data in chunks.\n",
      "        (Only valid with C parser)\n",
      "    memory_map : boolean, default False\n",
      "        If a filepath is provided for `filepath_or_buffer`, map the file object\n",
      "        directly onto memory and access the data directly from there. Using this\n",
      "        option can improve performance because there is no longer any I/O overhead.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    result : DataFrame or TextParser\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Importing using `pd.read_excel()`\n",
    "It is common for data to be stored in Excel worksheets (provided the data is not too large).  You can easily export data from Excel to CSV or Tab delimited formats but Pandas provides  the `pd.read_excel` function to save you the time.  We will not practice loading data from Excel, but take a look at the [`read_excel` online documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_excel in module pandas.io.excel:\n",
      "\n",
      "read_excel(io, sheet_name=0, header=0, names=None, index_col=None, usecols=None, squeeze=False, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skiprows=None, nrows=None, na_values=None, parse_dates=False, date_parser=None, thousands=None, comment=None, skipfooter=0, convert_float=True, **kwds)\n",
      "    Read an Excel table into a pandas DataFrame\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    io : string, path object (pathlib.Path or py._path.local.LocalPath),\n",
      "        file-like object, pandas ExcelFile, or xlrd workbook.\n",
      "        The string could be a URL. Valid URL schemes include http, ftp, s3,\n",
      "        and file. For file URLs, a host is expected. For instance, a local\n",
      "        file could be file://localhost/path/to/workbook.xlsx\n",
      "    sheet_name : string, int, mixed list of strings/ints, or None, default 0\n",
      "    \n",
      "        Strings are used for sheet names, Integers are used in zero-indexed\n",
      "        sheet positions.\n",
      "    \n",
      "        Lists of strings/integers are used to request multiple sheets.\n",
      "    \n",
      "        Specify None to get all sheets.\n",
      "    \n",
      "        str|int -> DataFrame is returned.\n",
      "        list|None -> Dict of DataFrames is returned, with keys representing\n",
      "        sheets.\n",
      "    \n",
      "        Available Cases\n",
      "    \n",
      "        * Defaults to 0 -> 1st sheet as a DataFrame\n",
      "        * 1 -> 2nd sheet as a DataFrame\n",
      "        * \"Sheet1\" -> 1st sheet as a DataFrame\n",
      "        * [0,1,\"Sheet5\"] -> 1st, 2nd & 5th sheet as a dictionary of DataFrames\n",
      "        * None -> All sheets as a dictionary of DataFrames\n",
      "    \n",
      "    sheetname : string, int, mixed list of strings/ints, or None, default 0\n",
      "    \n",
      "        .. deprecated:: 0.21.0\n",
      "           Use `sheet_name` instead\n",
      "    \n",
      "    header : int, list of ints, default 0\n",
      "        Row (0-indexed) to use for the column labels of the parsed\n",
      "        DataFrame. If a list of integers is passed those row positions will\n",
      "        be combined into a ``MultiIndex``. Use None if there is no header.\n",
      "    names : array-like, default None\n",
      "        List of column names to use. If file contains no header row,\n",
      "        then you should explicitly pass header=None\n",
      "    index_col : int, list of ints, default None\n",
      "        Column (0-indexed) to use as the row labels of the DataFrame.\n",
      "        Pass None if there is no such column.  If a list is passed,\n",
      "        those columns will be combined into a ``MultiIndex``.  If a\n",
      "        subset of data is selected with ``usecols``, index_col\n",
      "        is based on the subset.\n",
      "    parse_cols : int or list, default None\n",
      "    \n",
      "        .. deprecated:: 0.21.0\n",
      "           Pass in `usecols` instead.\n",
      "    \n",
      "    usecols : int or list, default None\n",
      "        * If None then parse all columns,\n",
      "        * If int then indicates last column to be parsed\n",
      "        * If list of ints then indicates list of column numbers to be parsed\n",
      "        * If string then indicates comma separated list of Excel column letters and\n",
      "          column ranges (e.g. \"A:E\" or \"A,C,E:F\").  Ranges are inclusive of\n",
      "          both sides.\n",
      "    squeeze : boolean, default False\n",
      "        If the parsed data only contains one column then return a Series\n",
      "    dtype : Type name or dict of column -> type, default None\n",
      "        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32}\n",
      "        Use `object` to preserve data as stored in Excel and not interpret dtype.\n",
      "        If converters are specified, they will be applied INSTEAD\n",
      "        of dtype conversion.\n",
      "    \n",
      "        .. versionadded:: 0.20.0\n",
      "    \n",
      "    engine: string, default None\n",
      "        If io is not a buffer or path, this must be set to identify io.\n",
      "        Acceptable values are None or xlrd\n",
      "    converters : dict, default None\n",
      "        Dict of functions for converting values in certain columns. Keys can\n",
      "        either be integers or column labels, values are functions that take one\n",
      "        input argument, the Excel cell content, and return the transformed\n",
      "        content.\n",
      "    true_values : list, default None\n",
      "        Values to consider as True\n",
      "    \n",
      "        .. versionadded:: 0.19.0\n",
      "    \n",
      "    false_values : list, default None\n",
      "        Values to consider as False\n",
      "    \n",
      "        .. versionadded:: 0.19.0\n",
      "    \n",
      "    skiprows : list-like\n",
      "        Rows to skip at the beginning (0-indexed)\n",
      "    nrows : int, default None\n",
      "        Number of rows to parse\n",
      "    \n",
      "        .. versionadded:: 0.23.0\n",
      "    \n",
      "    na_values : scalar, str, list-like, or dict, default None\n",
      "        Additional strings to recognize as NA/NaN. If dict passed, specific\n",
      "        per-column NA values. By default the following values are interpreted\n",
      "        as NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
      "        '1.#IND', '1.#QNAN', 'N/A', 'NA', 'NULL', 'NaN', 'n/a', 'nan',\n",
      "        'null'.\n",
      "    keep_default_na : bool, default True\n",
      "        If na_values are specified and keep_default_na is False the default NaN\n",
      "        values are overridden, otherwise they're appended to.\n",
      "    verbose : boolean, default False\n",
      "        Indicate number of NA values placed in non-numeric columns\n",
      "    thousands : str, default None\n",
      "        Thousands separator for parsing string columns to numeric.  Note that\n",
      "        this parameter is only necessary for columns stored as TEXT in Excel,\n",
      "        any numeric columns will automatically be parsed, regardless of display\n",
      "        format.\n",
      "    comment : str, default None\n",
      "        Comments out remainder of line. Pass a character or characters to this\n",
      "        argument to indicate comments in the input file. Any data between the\n",
      "        comment string and the end of the current line is ignored.\n",
      "    skip_footer : int, default 0\n",
      "    \n",
      "        .. deprecated:: 0.23.0\n",
      "           Pass in `skipfooter` instead.\n",
      "    skipfooter : int, default 0\n",
      "        Rows at the end to skip (0-indexed)\n",
      "    convert_float : boolean, default True\n",
      "        convert integral floats to int (i.e., 1.0 --> 1). If False, all numeric\n",
      "        data will be read in as floats: Excel stores all numbers as floats\n",
      "        internally\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    parsed : DataFrame or Dict of DataFrames\n",
      "        DataFrame from the passed in Excel file.  See notes in sheet_name\n",
      "        argument for more information on when a Dict of Dataframes is returned.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    An example DataFrame written to a local file\n",
      "    \n",
      "    >>> df_out = pd.DataFrame([('string1', 1),\n",
      "    ...                        ('string2', 2),\n",
      "    ...                        ('string3', 3)],\n",
      "    ...                       columns=['Name', 'Value'])\n",
      "    >>> df_out\n",
      "          Name  Value\n",
      "    0  string1      1\n",
      "    1  string2      2\n",
      "    2  string3      3\n",
      "    >>> df_out.to_excel('tmp.xlsx')\n",
      "    \n",
      "    The file can be read using the file name as string or an open file object:\n",
      "    \n",
      "    >>> pd.read_excel('tmp.xlsx')\n",
      "          Name  Value\n",
      "    0  string1      1\n",
      "    1  string2      2\n",
      "    2  string3      3\n",
      "    \n",
      "    >>> pd.read_excel(open('tmp.xlsx','rb'))\n",
      "          Name  Value\n",
      "    0  string1      1\n",
      "    1  string2      2\n",
      "    2  string3      3\n",
      "    \n",
      "    Index and header can be specified via the `index_col` and `header` arguments\n",
      "    \n",
      "    >>> pd.read_excel('tmp.xlsx', index_col=None, header=None)\n",
      "         0        1      2\n",
      "    0  NaN     Name  Value\n",
      "    1  0.0  string1      1\n",
      "    2  1.0  string2      2\n",
      "    3  2.0  string3      3\n",
      "    \n",
      "    Column types are inferred but can be explicitly specified\n",
      "    \n",
      "    >>> pd.read_excel('tmp.xlsx', dtype={'Name':str, 'Value':float})\n",
      "          Name  Value\n",
      "    0  string1    1.0\n",
      "    1  string2    2.0\n",
      "    2  string3    3.0\n",
      "    \n",
      "    True, False, and NA values, and thousands separators have defaults,\n",
      "    but can be explicitly specified, too. Supply the values you would like\n",
      "    as strings or lists of strings!\n",
      "    \n",
      "    >>> pd.read_excel('tmp.xlsx',\n",
      "    ...               na_values=['string1', 'string2'])\n",
      "          Name  Value\n",
      "    0      NaN      1\n",
      "    1      NaN      2\n",
      "    2  string3      3\n",
      "    \n",
      "    Comment lines in the excel input file can be skipped using the `comment` kwarg\n",
      "    \n",
      "    >>> df = pd.DataFrame({'a': ['1', '#2'], 'b': ['2', '3']})\n",
      "    >>> df.to_excel('tmp.xlsx', index=False)\n",
      "    >>> pd.read_excel('tmp.xlsx')\n",
      "        a  b\n",
      "    0   1  2\n",
      "    1  #2  3\n",
      "    \n",
      "    >>> pd.read_excel('tmp.xlsx', comment='#')\n",
      "       a  b\n",
      "    0  1  2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.read_excel) # syntax: df = pd.read_excel('File.xlsx', sheetname='Sheet1'), where the first parameter is the filename and the second parameter is the sheet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Explore your Data\n",
    "There are a variety of functions for exploring data once you have imported it. Here we will discuss `pd.head`, `pd.tail`, `pd.sample`, and `pd.describe`.\n",
    "\n",
    "### 4.1 The `pd.head` function\n",
    "\n",
    "As with any Python variable, you can display its contents by simply typing the variable name and pressing the \"enter\" key.  Howevever, for very large data files this may not be desired.  The data used in this tutorial is not extremly large but this often not the case.\n",
    "\n",
    "To easily examine your the first set of rows in your DataFrame, use the `head()` function of the `pd.DataFrame` object. By deafult it displays the first 5 rows. Execute the following cells to view the first five rows of the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iris_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d721b3361508>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0miris_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'iris_df' is not defined"
     ]
    }
   ],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an excellent way to check if you created your `pd.DataFrame` object correctly.  You can change the number of rows returned by providing an argument. The following requests the first 10 rows of the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The `pd.tail` function\n",
    "Simliar to the `pd.head` function, the `pd.tail` function lets you peak at the last 5 rows of data.  To demonstrate, execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The `pd.sample` function\n",
    "The `pd.sample` function selects a random set of rows to display. By default it only selects 1 row, we can provide the number of rows we would like (e.g. 5) by providing the number as an arugment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat execution of the function to see a different set of randomly selected rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The `pd.describe` function\n",
    "The `pd.describe` function helps demonstrate the power of Pandas over Python dictionaries and Numpy arrays.  It provides a summary of statistics for the data frame in one simple function call. These statistics include the count, mean, standard deviation, min, max and quartiles.\n",
    "\n",
    "Execute the following cell to demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4a: Use head, tail and sample with the iris data frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Accessing Data\n",
    "\n",
    "In this section we will learn to retreive values, slice the data frame, and use column and row labels (rather than numeric indexes) to find data. \n",
    "\n",
    "### 5.1 Viewing column and row labels.\n",
    "Before we delve into retreiving data, lets first look at some properties of data frames for retrieving the dataframe column and row names.  This is because we often want to work with, or check the indexes and columns of a data frame using their names.  First, to retrieve the columns names we can use the `columns` property of the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similary, we can get a description of the index using the `index` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that here our index is numeric and runs from 0 to 150, incremented by 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5a: Display the columns and indexes of the iris data frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Get all the values\n",
    "\n",
    "Perhaps you would like to extract the contents of the DataFrame into a 2-dimensional Python list. You will lose the power of Pandas, but you should know how to do this. There are two ways to extract values: the new Pandas v0.24 way and the older way.  \n",
    "\n",
    "This following code example used to be the standard way of getting the values of a dataframe as a `numpy` array:\n",
    "\n",
    "```python\n",
    "my_array = df.values\n",
    "```\n",
    "\n",
    "This is depreciated in version 0.24 of `pandas` to instead require the use of `to_numpy()`.\n",
    "\n",
    "```python\n",
    "my_array = df.to_numpy()\n",
    "```\n",
    "\n",
    "You can verify what version of `pandas` you have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5b: Check the version of `pandas` you have, then convert a data frame to a numpy array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Basic Selections\n",
    "Pandas provides a variety of row/column selectors that are similar to the way that lists and arrays are indxed in Python and Numpy. This tutorial will introduce those. However, please note:\n",
    "> While standard Python / Numpy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods, '.at', '.iat', '.loc' and '.iloc'.\n",
    "\n",
    "This tutorial will also descirbe the `.at`, `.iat`, `.loc` and `.iloc` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Get a column\n",
    "The easiest way to select a single column is to use the column label! When in doubt, use the `.column` property of the DataFrame to find the list of column labels.  Using the column label will result in the column being returned as a Series object.  For example, if we want the `sepal_length` column of the iris data frame we can execute the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slength = iris_df['sepal_length']\n",
    "slength.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe, we can use the `.head` function to get the first few rows of a Series as well as a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Get rows (slicing)\n",
    "Similar to slicing of Python lists, we can do the same with Pandas DataFrames:\n",
    "\n",
    "```python\n",
    "iris_df[start:end]\n",
    "```\n",
    "\n",
    "As a reminder, indexing of a list in Python uses 0-based indexing and negative numbers index in reverse order:\n",
    "\n",
    "```\n",
    " +---+---+---+---+---+---+\n",
    " | P | y | t | h | o | n |\n",
    " +---+---+---+---+---+---+\n",
    "   0   1   2   3   4   5  \n",
    "  -6  -5  -4  -3  -2  -1\n",
    "```\n",
    "\n",
    "**Important:** Recall the recommendation to *not* using this type of indexing (slicing) in production with very large data. Instead use other provided functions (`.loc`, `iloc`, etc). However, to practice, let's get the first row of the iris dataset by slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets get the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df[0:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Selecting with `df.loc` and `df.iloc`\n",
    "\n",
    "Aside from the column name and slicing (as shown in the previous sections) Pandas provides the `df.loc` and `df.iloc` accessor.  These accessors perform better for selecting subsets of the dataframe. The `.loc` accessor is used for slicing by columns and `.iloc` is used for slicing by rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 Using `.loc`\n",
    "\n",
    "The `.loc` accessor allows for the slicing of the data frame by using the labels of either rows and columns.  \n",
    "\n",
    "The syntax is as follows:\n",
    "\n",
    "```python\n",
    "df.loc[row_start:row_end, column_start:column_end]\n",
    "```\n",
    "\n",
    "Remember, because the rows in the iris data frame are indexed using integers, we can use `.loc` to get rows using the integer values.  To get the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first two rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.loc[0:1]  # Get the first two rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that when only one row is returned that a Series is provided.  When multiple rows are returned then a new DataFrame object is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, recall that we constructed a 6x4 dataframe in section 2.3 that was filled with random values and we set the row names using datetime values. The data frame was named `df`.  As a reminder here are the first 2 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data frame the rows are indexed using strings that represent dates. We can therefore use those labels to slice the dataframe.  The following extracts the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2013-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can get the first two rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2013-01-01':'2013-01-02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.2 Using `.loc` to get columns\n",
    "Remember for our iris data frame the column indexes have the names as provided by the `.columns` property.\n",
    "\n",
    "```python\n",
    "iris_df.columns\n",
    "Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n",
    "       'species'],\n",
    "      dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these names to retrive values for specific columns. For example, the code below extracts rows 4 through 6 and the columns 'sepal_width' and 'petal_width':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.loc[4:6, 'sepal_width': 'petal_width']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of the colon in the code above between 'sepal_width' and 'petal_width' implies a range. Therefore every column between those two are included. This is why 'petal_length' appears in the output.  If we wanted to specifically select only the two columns and none other, we could provide the names in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.loc[4:6, ['sepal_width', 'petal_width']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that now we only have 'sepal_width', and 'petal_width' in the resulting data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we can limit the rows to only those specified in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.loc[[4,6], ['sepal_width', 'petal_width']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5c: Make selections with `loc`\n",
    "\n",
    "+ Select a single item with `at`.\n",
    "+ Select a row slice with `loc`.\n",
    "+ Select a row and column slice with `loc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 Using `.iloc`\n",
    "\n",
    "The `.iloc` accessor allows us to slice the data frame using an integer index, regardless of what the index (or column) labels actually are.\n",
    "\n",
    "The syntax is the same as `.loc` with the exception of that integers are used instead of labels:\n",
    "\n",
    "```python\n",
    "df.iloc[row_start:row_end, column_start:column_end]\n",
    "```\n",
    "To retrieve the first row of the iris data we use the numeric index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.iloc[0]  # Get the first row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use any combination of integer indexes to get a subset of rows and columns.  The following examples retieves rows 2 to 5, and excludes the 'species' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.iloc[2:5, :-1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5d: Make selections with `iloc` and `iat`.\n",
    "\n",
    "+ Select a single item with `iat`.\n",
    "+ Select a row slice with `iloc`.\n",
    "+ Select a row and column slice with `iloc`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 \"Fancy\" indexing with `.where`\n",
    "\n",
    "Pandas allows us to use boolean values to extract specific rows of data that meet certain conditions.To clarify this, first examine what occurs when we apply a condition to a data frame.  Recall the data frame we created in section 2.2.1.  As a reminder execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply a condition to the data frame to find all values > 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 > 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the resulting data frame contains `True` and `False` values in place of every value that did or did not meet the condition.  We can use this new data frame to subset the original dataframe.\n",
    "\n",
    "Lets consider the iris dataset for a more realistic example.  Suppose we want to find all rows with a `sepal_length` greater than 5.8. We know from the call to `describe` that the mean is approximately 5.8. We can do so with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = iris_df['sepal_length'] > 5.8\n",
    "iris_df[condition].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could rewrite the two lines above to combine them into a single line with identical results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df[iris_df['sepal_length'] > 5.8].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5e: Boolean Indexing\n",
    "\n",
    "Create subsets using boolean indexes that:\n",
    "+ Use one boolean operator.\n",
    "+ Use two boolean operators.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
